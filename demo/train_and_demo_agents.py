# train_and_demo_agents.py
import os,sys
import numpy as np
import matplotlib.pyplot as plt
import torch
import imageio

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from environments.setup_env import create_highway_env
from agents.rainbow_agent import RainbowAgent
from agents.a3c_agent import run_demo, ActorCritic

os.makedirs("demo/sample_results", exist_ok=True)

######## Rainbow DQN FULL TRAIN ########

print("Training Rainbow DQN agent (this will take a while)...")
env = create_highway_env()
state_dim = np.prod(env.observation_space.shape)
action_dim = env.action_space.n
rainbow = RainbowAgent(state_dim, action_dim)

num_episodes = 1000  # <-- Increase for more learning!
reward_history = []
for episode in range(num_episodes):
    s, _ = env.reset()
    s = s.flatten()
    total = 0
    for _ in range(500):
        a = rainbow.act(s)
        ns, r, done, truncated, _ = env.step(a)
        ns = ns.flatten()
        rainbow.memory.push(s, a, r, ns, done)
        rainbow.learn()
        s = ns
        total += r
        if done or truncated:
            break
    rainbow.update_target()
    reward_history.append(total)
    if (episode+1) % 100 == 0:
        print(f"Rainbow episode {episode+1}: reward {total}")

# Save model
torch.save(rainbow.policy_net.state_dict(), "demo/sample_results/rainbow_trained.pth")
np.savetxt("demo/sample_results/rainbow_train_rewards.csv", reward_history, delimiter=",")
plt.figure(figsize=(8,4))
plt.plot(reward_history)
plt.title("Rainbow Training")
plt.savefig("demo/sample_results/rainbow_train_curve.png")
print("Saved Rainbow DQN model and training curve.")

# Demo rollout with trained agent
print("Generating Rainbow DQN demo GIF...")
rainbow.policy_net.load_state_dict(torch.load("demo/sample_results/rainbow_trained.pth"))
env = create_highway_env(render_mode="rgb_array")
frames = []
s, _ = env.reset()
s = s.flatten()
for _ in range(200):
    a = rainbow.act(s, epsilon=0.0)  # Greedy for best performance
    ns, _, done, truncated, _ = env.step(a)
    ns = ns.flatten()
    frame = env.render()
    frames.append(frame)
    s = ns
    if done or truncated:
        break
imageio.mimsave("demo/sample_results/rainbow_demo.mp4", frames, fps=20)
print("Saved Rainbow DQN demo MP4.")

######## A3C FULL TRAIN ########

print("Training A3C agent (this will also take a while)...")
# NOTE: This uses your run_demo Ã  la a3c_agent.py (single-threaded for simplicity)
train_rewards = run_demo(env_id="highway-v0", episodes=1000)
np.savetxt("demo/sample_results/a3c_train_rewards.csv", train_rewards, delimiter=",")
plt.figure(figsize=(8,4))
plt.plot(train_rewards)
plt.title("A3C Training")
plt.savefig("demo/sample_results/a3c_train_curve.png")
print("Saved A3C training curve.")

# Save trained weights for demo
# (A3C demo API does not currently provide trained ActorCritic object interface directly,
# but if you update run_demo to save/load weights, you can leverage this step to get a greedy demo)

# For demo, just run another quick roll-out (assume last-trained policy held in memory):
from agents.a3c_agent import Agent
env = create_highway_env(render_mode="rgb_array")
input_shape = env.observation_space.shape
n_actions = env.action_space.n
global_model = ActorCritic(input_shape, n_actions)
manager = torch.multiprocessing.Manager()
global_ep_idx = manager.Value('i', 0)
res_queue = manager.Queue()
demo_agent = Agent(global_model, input_shape, n_actions, 0.99, 1e-4, "demo", global_ep_idx, res_queue, "highway-v0")

frames = []
s, _ = env.reset()
for _ in range(200):
    a = demo_agent.act(s)
    ns, _, done, truncated, _ = env.step(a)
    frame = env.render()
    frames.append(frame)
    s = ns
    if done or truncated:
        break
imageio.mimsave("demo/sample_results/a3c_demo.mp4", frames, fps=20)
print("Saved A3C demo MP4.")

######## Show user where results are ########
print("\n**Training and demo outputs saved in demo/sample_results/**")
print("You can now use these MP4s and training curves in your Streamlit app and README for impressive results!")
